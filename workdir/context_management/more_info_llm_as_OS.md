Software 3.0 is introducing even deeper changes than the previous revolution. We program in natural language by engaging in dialogue with a model. Instead of writing code (as in 1.0) or curating data (as in 2.0), we simply describe the problem and communicate the expected outcome. We no longer write code but synthesize it on the fly through a powerful pre-trained model. The role of humans is changing again — we are becoming architects, editors and AI psychologists who must precisely formulate commands and critically assess the results. This drastically lowers the entry threshold, but at the same time raises new fundamental questions. It’s no longer just about whether the code works, but about how to trust something that was created in response to a few casual remarks. How to ensure its reliability, safety and predictability in critical applications?


LLM as the new mainframe computer
To help us understand this change, Karpathy suggests an intriguing analogy. According to it, today a large language model (LLM) functions like an entire computer. The very model, for instance GPT-4, is equivalent to a processor (CPU) — the central unit that executes all commands. The context window serves as RAM, storing current instructions and data required for operations. Meanwhile, tools and plugins, such as internet access, a calculator or a code interpreter, function like peripherals and drivers that allow the processor to communicate with the external world. In this metaphor, applications like Cursor or Perplexity become portable programs that can be run on different “operating systems” (AI models).

In Karpathy’s opinion, the current situation resembles the 1960s and the era of mainframe computers: powerful, centralized machines that users connected to via simple terminals and shared computing time among themselves. Today, these terminals are chat windows, and we share computing resources from OpenAI, Google or Anthropic.

Model psychology
One of the biggest challenges is the deeply unusual “psychology” of these new computers. Karpathy describes it as a fascinating and paradoxical combination of two film characters. On one hand, LLMs are like Raymond from “Rain Man” — brilliant savants with photographic, flawless short-term memory. Within an active contextual window, they can instantly process and correlate thousands of pages of text and solve problems in a way that is inaccessible to the human mind. They won’t forget a single detail that was provided to them. This genius, however, is fleeting and limited to the present moment. On the other hand, they resemble Leonard from “Memento” — they suffer from a severe form of amnesia. When context disappears, memory also fades. They lack a permanent, stable understanding of the world and are incredibly susceptible to suggestions. And since they are essentially prediction, not reasoning machines, they can be easily deceived. Just one subtly misleading hint is enough for them to start hallucinating and confidently generate convincing, yet completely fabricated information. Not even lies, but statistical echoes that sound correct and very credible, making them particularly insidious. Therefore, the only effective strategy to work with them is a simple rule: generate, then verify. The person must remain the pilot, leveraging the machine’s remarkable speed, but also providing critical thinking and ultimate judgment.

Autonomy slider and vibe coding
Instead of striving for complete, often fragile AI autonomy, Karpathy promotes a much more pragmatic idea — the “autonomy slider”. The idea is to create systems that work like the exoskeleton from the movie “Iron Man”, not replacing humans, but enhancing their abilities. In this model, AI becomes a powerful partner: it generates code, writes draft texts, analyzes data sets or creates graphic designs. But people remain at the core when it comes to making decisions. With one click, we accept, modify or reject the proposal. We have full control over the final outcome. This approach builds trust and allows users to gradually delegate more tasks.