
save-token.sh
-> für current conv
-> req evaluation
count-token -> als llm task außerhalb convers?

----

_active_conversations[(user_hash, channel)] -->  multiple ConversationManager instances per container
-> nötig für subagent /loop

----


zweites tool: send file via backend to user instead of llm answer -> file straight from container to UI

---


save conversation in "last chats" autom 


---
komplette conv hist zw fastapi und openai zu jeweiligen container loggen !! 
---






Port Mapping for ssh -> user can ssh into own container:
firewallöffnen
createuser script ändern: port + key -> beide keys auf own volume laden
dockercompose: custom port
list notwendig?




conversation turn counter -> gegen ende trim prompt inject -> antr/openai api funktionaliry dafür nutzen
token counter



terraform ebs in userdata script 




redis streams   -> ey  



UI:
-text snippets markieren und mit Tags vershen + user comment dazu -> was sieht llm? wie gui comments?

- click_button einfach als "tag" ans llm schicken während conv? 

- WYSIWYG  editor




---

self eval von außen während conv, -> inject pruning/declutter P


---

reference chat history during summarization !!

long tool responses into files -> oder fork dirket?






Dockerhost Fastapi Endpoint -> priv api ??




remove Pidgin english   -> save tken vs weaker output



add in req: command boot: date -> in jeden file bei file edit